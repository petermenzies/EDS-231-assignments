---
title: "Topic 6: Topic Analysis"
author: Peter Menzies
date: "5/8/22"
output: pdf_document
---

```{r packages, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(here)
library(pdftools)
library(quanteda)
library(tm)
library(topicmodels)
library(ldatuning)
library(tidyverse)
library(tidytext)
library(reshape2)

``` 

Load the data

```{r data}
comments_df <- readRDS(here("data", "comments_df.RDS"))
```

Now we'll build and clean the corpus

```{r corpus}
epa_corp <- corpus(x = comments_df, text_field = "text")
epa_corp.stats <- summary(epa_corp)
head(epa_corp.stats, n = 25)
toks <- tokens(epa_corp, remove_punct = TRUE, remove_numbers = TRUE)
#I added some project-specific stop words here
add_stops <- c(stopwords("en"),"environmental", "justice", "ej", "epa", "public", "comment")
toks1 <- tokens_select(toks, pattern = add_stops, selection = "remove")

```

And now convert to a document-feature matrix

```{r dfm}
dfm_comm<- dfm(toks1, tolower = TRUE)
dfm <- dfm_wordstem(dfm_comm)
dfm <- dfm_trim(dfm, min_docfreq = 2) #remove terms only appearing in one doc (min_termfreq = 10)

print(head(dfm))

#remove rows (docs) with all zeros
sel_idx <- slam::row_sums(dfm) > 0 
dfm <- dfm[sel_idx, ]
#comments_df <- dfm[sel_idx, ]


```

We somehow have to come up with a value for k,the number of latent topics present in the data. How do we do this? There are multiple methods. Let's use what we already know about the data to inform a prediction. The EPA has 9 priority areas: Rulemaking, Permitting, Compliance and Enforcement, Science, States and Local Governments, Federal Agencies, Community-based Work, Tribes and Indigenous People, National Measures. Maybe the comments correspond to those areas?

```{r LDA_modeling}
k <- 9 

topicModel_k9 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = FALSE))
#nTerms(dfm_comm) 

tmResult <- posterior(topicModel_k9)
attributes(tmResult)
#nTerms(dfm_comm)   
beta <- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms# lengthOfVocab
terms(topicModel_k9, 10)
```

Some of those topics seem related to the cross-cutting and additional topics identified in the EPA's response to the public comments:

1\. Title VI of the Civil Rights Act of 1964

2.[EJSCREEN](https://www.epa.gov/ejscreen/download-ejscreen-data)

3\. climate change, climate adaptation and promoting greenhouse gas reductions co-benefits

4\. overburdened communities and other stakeholders to meaningfully, effectively, and transparently participate in aspects of EJ 2020, as well as other agency processes

5\. utilize multiple Federal Advisory Committees to better obtain outside environmental justice perspectives

6\. environmental justice and area-specific training to EPA staff

7\. air quality issues in overburdened communities

So we could guess that there might be a 16 topics (9 priority + 7 additional). Or we could calculate some metrics from the data.

```{r LDA_again}
#
result <- FindTopicsNumber(
  dfm,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = FALSE
)

FindTopicsNumber_plot(result)

k <- 7

topicModel_k7 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = FALSE))

tmResult <- posterior(topicModel_k7)
terms(topicModel_k7, 10)
theta <- tmResult$topics
beta <- tmResult$terms
vocab <- (colnames(beta))

```

There are multiple proposed methods for how to measure the best k value. You can [go down the rabbit hole here](https://rpubs.com/siri/ldatuning)

```{r top_terms_topic}

comment_topics <- tidy(topicModel_k7, matrix = "beta")

top_terms <- comment_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

```{r plot_top_terms}

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

Let's assign names to the topics so we know what we are working with. We can name them by their top terms

```{r topic_names}
top5termsPerTopic <- terms(topicModel_k7, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
```

We can explore the theta matrix, which contains the distribution of each topic over each document

```{r topic_dists}
exampleIds <- c(1, 2, 3)
N <- length(exampleIds)

#lapply(epa_corp[exampleIds], as.character) #uncomment to view example text
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document=factor(1:N)), variable.name = "topic", id.vars = "document")  
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)
```

Here's a neat JSON-based model visualizer

```{r LDAvis, eval=FALSE, warning=FALSE}
library(LDAvis)
library("tsne")
svd_tsne <- function(x) tsne(svd(x)$u)
json <- createJSON(
  phi = tmResult$terms, 
  theta = tmResult$topics, 
  doc.length = rowSums(dfm), 
  vocab = colnames(dfm), 
  term.frequency = colSums(dfm),
  mds.method = svd_tsne,
  plot.opts = list(xlab="", ylab="")
)
serVis(json)

```


## Analysis continued

### 14 topics

```{r, fig.height=6}
k <- 14 

topicModel_k14 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = FALSE))

tmResult <- posterior(topicModel_k14)
attributes(tmResult)
beta <- tmResult$terms   
dim(beta)                
terms(topicModel_k14, 10)


comment_topics <- tidy(topicModel_k14, matrix = "beta")

top_terms <- comment_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

### 10 topics

```{r}
k <- 10 

topicModel_k10 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = FALSE))

tmResult <- posterior(topicModel_k10)
attributes(tmResult)
beta <- tmResult$terms   
dim(beta)                
terms(topicModel_k10, 10)


comment_topics <- tidy(topicModel_k10, matrix = "beta")

top_terms <- comment_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

### 8 Topics

```{r}
k <- 8 

topicModel_k8 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = FALSE))

tmResult <- posterior(topicModel_k8)
attributes(tmResult)
beta <- tmResult$terms  
dim(beta)                
terms(topicModel_k8, 10)


comment_topics <- tidy(topicModel_k8, matrix = "beta")

top_terms <- comment_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

## Best value for k

Based on the Deveaud2014 metric in the `FindTopicsNumber()` analysis, I chose to try 10, 14, and 8 as possible numbers of topics. I assessed these models by looking at the frequency of top terms in each topic and with the `LDAvis` app. After running these additional models, I think that 8 topics has been the most successful so far. In my opinion it seems like when more than 8 topics are formed, they start to become more redundant and the lines between them start to blur. In part, I based this on looking at the top words in each supposed topic and feeling out how cohesive and unique each was. Using `LDAvis`, it appears using 8 topics creates a fairly equidistant spacing between the topics---as the topics increase beyond this amount, certain topics start to become closer to one another. This choice would align fairly well with Deveaud2014 metric, as 8 was one of the topic numbers with a higher value, albeit not among the very highest values. 

</br>
</br>


